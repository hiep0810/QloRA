{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiepdvh/miniconda3/envs/rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262410240 || all params: 262410240 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'TheBloke/Mistral-7B-Instruct-v0.2-GPTQ'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',  # automatically figures out how to best use CPU + GPU for loading model\n",
    "    trust_remote_code=False,  # prevents running custom model files on your machine\n",
    "    revision='main',\n",
    ")  # which version of model to use in repo\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f'trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}'\n",
    "    )\n",
    "\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Great content, thank you! [/INST] I'm glad you found the content helpful! If you have any specific questions or topics you'd like me to cover in the future, feel free to ask. I'm here to help.\n",
      "\n",
      "In the meantime, I'd be happy to answer any questions you have about the content I've already provided. Just let me know which article or blog post you're referring to, and I'll do my best to provide you with accurate and up-to-date information.\n",
      "\n",
      "Thanks for reading, and I look forward to helping you with any questions you may have!</s>\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # model in evaluation mode (dropout modules are deactivated)\n",
    "\n",
    "# craft prompt\n",
    "comment = 'Great content, thank you!'\n",
    "prompt = f\"\"\"[INST] {comment} [/INST]\"\"\"\n",
    "\n",
    "# tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "# generate output\n",
    "outputs = model.generate(input_ids=inputs['input_ids'].to('cuda'), max_new_tokens=140)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()  # model in training mode (dropout modules are activated)\n",
    "\n",
    "# enable gradient check pointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# enable quantized training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2097152 || all params: 264507392 || trainable%: 0.7928519441906561\n"
     ]
    }
   ],
   "source": [
    "# LoRA config\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q_proj'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    ")\n",
    "\n",
    "# LoRA trainable version of model\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f'trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}'\n",
    "    )\n",
    "\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = load_dataset('shawhin/shawgpt-youtube-comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 8537.50 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2531.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# create tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples['example']\n",
    "\n",
    "    # tokenize and truncate text\n",
    "    tokenizer.truncation_side = 'left'\n",
    "    tokenized_inputs = tokenizer(text, return_tensors='np', truncation=True, max_length=512)\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# tokenize training and validation datasets\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['example', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# data collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 2e-4\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "\n",
    "# define training arguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir='shawgpt-ft',\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    fp16=True,\n",
    "    optim='paged_adamw_8bit',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# configure trainer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m----> 4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtokenized_data\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      7\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_data' is not defined"
     ]
    }
   ],
   "source": [
    "# configure trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['test'],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# train model\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n",
    "\n",
    "# renable warnings\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralSdpaAttention(\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "              (k_proj): QuantLinear()\n",
       "              (o_proj): QuantLinear()\n",
       "              (q_proj): lora.QuantLinear(\n",
       "                (base_layer): QuantLinear()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (quant_linear_module): QuantLinear()\n",
       "              )\n",
       "              (v_proj): QuantLinear()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (act_fn): SiLU()\n",
       "              (down_proj): QuantLinear()\n",
       "              (gate_proj): QuantLinear()\n",
       "              (up_proj): QuantLinear()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_data['train'],\n",
    "    shuffle=True,\n",
    "    batch_size=4,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "val_dataloder = DataLoader(\n",
    "    tokenized_data['test'],\n",
    "    batch_size=4,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([4, 209]), 'attention_mask': torch.Size([4, 209]), 'labels': torch.Size([4, 209])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "print({k: v.shape for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiepdvh/miniconda3/envs/rag/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/130. Loss: 4.336075305938721\n",
      "2/130. Loss: 4.097919940948486\n",
      "3/130. Loss: 4.206480979919434\n",
      "4/130. Loss: 4.165226459503174\n",
      "5/130. Loss: 4.2231526374816895\n",
      "6/130. Loss: 4.438895225524902\n",
      "7/130. Loss: 4.41405725479126\n",
      "8/130. Loss: 3.7434208393096924\n",
      "9/130. Loss: 4.538475513458252\n",
      "10/130. Loss: 4.301823139190674\n",
      "11/130. Loss: 4.640456676483154\n",
      "12/130. Loss: 4.439744472503662\n",
      "13/130. Loss: 4.172786235809326\n",
      "14/130. Loss: 4.447360992431641\n",
      "15/130. Loss: 4.042270183563232\n",
      "16/130. Loss: 4.320154666900635\n",
      "17/130. Loss: 4.1808905601501465\n",
      "18/130. Loss: 4.14156436920166\n",
      "19/130. Loss: 4.365614891052246\n",
      "20/130. Loss: 4.297572612762451\n",
      "21/130. Loss: 4.400135040283203\n",
      "22/130. Loss: 4.1769819259643555\n",
      "23/130. Loss: 4.057359218597412\n",
      "24/130. Loss: 4.255882263183594\n",
      "25/130. Loss: 4.404635429382324\n",
      "26/130. Loss: 4.550881385803223\n",
      "27/130. Loss: 4.52702522277832\n",
      "28/130. Loss: 4.540689468383789\n",
      "29/130. Loss: 4.124856948852539\n",
      "30/130. Loss: 4.315665245056152\n",
      "31/130. Loss: 4.03415584564209\n",
      "32/130. Loss: 4.269615650177002\n",
      "33/130. Loss: 4.183410167694092\n",
      "34/130. Loss: 4.094357967376709\n",
      "35/130. Loss: 4.230683326721191\n",
      "36/130. Loss: 4.053977966308594\n",
      "37/130. Loss: 4.455972194671631\n",
      "38/130. Loss: 4.61655855178833\n",
      "39/130. Loss: 4.188284397125244\n",
      "40/130. Loss: 4.083243370056152\n",
      "41/130. Loss: 4.553621768951416\n",
      "42/130. Loss: 3.984851837158203\n",
      "43/130. Loss: 4.213683605194092\n",
      "44/130. Loss: 4.473567962646484\n",
      "45/130. Loss: 4.434940814971924\n",
      "46/130. Loss: 3.8742690086364746\n",
      "47/130. Loss: 4.325438022613525\n",
      "48/130. Loss: 4.3846049308776855\n",
      "49/130. Loss: 4.440637588500977\n",
      "50/130. Loss: 4.358051776885986\n",
      "51/130. Loss: 4.276023864746094\n",
      "52/130. Loss: 4.464202404022217\n",
      "53/130. Loss: 4.088953018188477\n",
      "54/130. Loss: 4.352630615234375\n",
      "55/130. Loss: 4.251536846160889\n",
      "56/130. Loss: 4.094235897064209\n",
      "57/130. Loss: 4.184739112854004\n",
      "58/130. Loss: 4.370508193969727\n",
      "59/130. Loss: 4.100583553314209\n",
      "60/130. Loss: 4.543972492218018\n",
      "61/130. Loss: 3.8602283000946045\n",
      "62/130. Loss: 4.539926052093506\n",
      "63/130. Loss: 4.447306156158447\n",
      "64/130. Loss: 4.428760051727295\n",
      "65/130. Loss: 4.527088165283203\n",
      "66/130. Loss: 4.01822566986084\n",
      "67/130. Loss: 4.6962809562683105\n",
      "68/130. Loss: 4.410703182220459\n",
      "69/130. Loss: 4.357523441314697\n",
      "70/130. Loss: 4.221165180206299\n",
      "71/130. Loss: 4.246814250946045\n",
      "72/130. Loss: 4.158300399780273\n",
      "73/130. Loss: 3.9476447105407715\n",
      "74/130. Loss: 4.105560779571533\n",
      "75/130. Loss: 4.271146774291992\n",
      "76/130. Loss: 4.480198383331299\n",
      "77/130. Loss: 4.406745433807373\n",
      "78/130. Loss: 4.337094306945801\n",
      "79/130. Loss: 4.032039642333984\n",
      "80/130. Loss: 4.507203578948975\n",
      "81/130. Loss: 4.338443756103516\n",
      "82/130. Loss: 4.582242965698242\n",
      "83/130. Loss: 4.200987815856934\n",
      "84/130. Loss: 4.281070232391357\n",
      "85/130. Loss: 4.39141845703125\n",
      "86/130. Loss: 3.86458158493042\n",
      "87/130. Loss: 4.4607768058776855\n",
      "88/130. Loss: 4.311104774475098\n",
      "89/130. Loss: 4.314477920532227\n",
      "90/130. Loss: 4.159038066864014\n",
      "91/130. Loss: 4.344624042510986\n",
      "92/130. Loss: 4.330104351043701\n",
      "93/130. Loss: 4.320508003234863\n",
      "94/130. Loss: 4.047585487365723\n",
      "95/130. Loss: 4.009146213531494\n",
      "96/130. Loss: 4.262332916259766\n",
      "97/130. Loss: 4.134731292724609\n",
      "98/130. Loss: 4.313706874847412\n",
      "99/130. Loss: 4.468626499176025\n",
      "100/130. Loss: 4.522339820861816\n",
      "101/130. Loss: 4.285963535308838\n",
      "102/130. Loss: 4.5348381996154785\n",
      "103/130. Loss: 4.0236663818359375\n",
      "104/130. Loss: 4.417718410491943\n",
      "105/130. Loss: 4.336198329925537\n",
      "106/130. Loss: 4.253891468048096\n",
      "107/130. Loss: 3.7660958766937256\n",
      "108/130. Loss: 4.151914596557617\n",
      "109/130. Loss: 4.547510147094727\n",
      "110/130. Loss: 4.54450798034668\n",
      "111/130. Loss: 4.210642337799072\n",
      "112/130. Loss: 4.227190971374512\n",
      "113/130. Loss: 4.694304466247559\n",
      "114/130. Loss: 4.447818756103516\n",
      "115/130. Loss: 4.123199939727783\n",
      "116/130. Loss: 4.162821292877197\n",
      "117/130. Loss: 4.144579887390137\n",
      "118/130. Loss: 4.467517852783203\n",
      "119/130. Loss: 4.12225341796875\n",
      "120/130. Loss: 4.374607563018799\n",
      "121/130. Loss: 4.598940372467041\n",
      "122/130. Loss: 3.8296918869018555\n",
      "123/130. Loss: 4.037581443786621\n",
      "124/130. Loss: 4.255971908569336\n",
      "125/130. Loss: 4.3364973068237305\n",
      "126/130. Loss: 4.229833602905273\n",
      "127/130. Loss: 4.320230007171631\n",
      "128/130. Loss: 4.692894458770752\n",
      "129/130. Loss: 4.051163673400879\n",
      "130/130. Loss: 4.389050006866455\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq import exllama_set_max_input_length\n",
    "\n",
    "model = exllama_set_max_input_length(model, max_input_length=19750)\n",
    "\n",
    "progress = 1\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f'{progress}/{num_training_steps}. Loss: {loss}')\n",
    "        progress += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiepdvh/miniconda3/envs/rag/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('finetune-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is fat-tailedness? [/INST] Fat-tailedness is a statistical property of certain distributions where the tails of the distribution are heavier or fatter than what would be expected from a normal distribution. In other words, the probability of observing extreme values is higher than what\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # model in evaluation mode (dropout modules are deactivated)\n",
    "\n",
    "# craft prompt\n",
    "comment = 'What is fat-tailedness?'\n",
    "prompt = f\"\"\"[INST] {comment} [/INST]\"\"\"\n",
    "\n",
    "# tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "# generate output\n",
    "outputs = model.generate(input_ids=inputs['input_ids'].to('cuda'), max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
